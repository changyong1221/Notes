# 人工智能学习笔记

## 中国计算机学会推荐国际学术会议和期刊目录-2019

### 数据库/数据挖掘/内容检索

![image-20201105150020602](pictures/image-20201105150020602.png)

![image-20201105150111939](pictures/image-20201105150111939.png)

![image-20201105150138005](pictures/image-20201105150138005.png)

![image-20201105150157531](pictures/image-20201105150157531.png)

![image-20201105150214905](pictures/image-20201105150214905.png)

![image-20201105150228708](pictures/image-20201105150228708.png)

### 人工智能

![image-20201105150447989](pictures/image-20201105150447989.png)

![image-20201105150528660](pictures/image-20201105150528660.png)

![image-20201105150547340](pictures/image-20201105150547340.png)

![image-20201105150630713](pictures/image-20201105150630713.png)

![image-20201105150701691](pictures/image-20201105150701691.png)

![image-20201105150731362](pictures/image-20201105150731362.png)

![image-20201105150812178](pictures/image-20201105150812178.png)

![image-20201105150954327](pictures/image-20201105150954327.png)

![image-20201105151012689](pictures/image-20201105151012689.png)

![image-20201105151036801](pictures/image-20201105151036801.png)

![image-20201105151056959](pictures/image-20201105151056959.png)

## SCI（科学引文索引）

Science Citation Index，简称 SCI，由美国科学信息研究所（Institute for Scientific Information, 简称 ISI）在美国费城创办的引文数据库，创始人为情报专家尤金·加菲尔德。目前SCI由信息提供商汤森路透公司（Thomson Reuters）负责运营。

世界著名的三大科学统计与科学评价检索系统——**SCI**（科学引文索引）、**EI**（工程索引）、**ISTP**（科技会议录索引）中，SCI 最为重要。被SCI收录的学术期刊称为SCI期刊，收录于SCI期刊的科技论文称为SCI论文。它以布拉德福(S. C. Bradford)文献离散律理论、以加菲尔德(E. Garfield)引文分析理论为主要基础，通过论文的被引用频次等的统计，对学术期刊和科研成果进行多方位的评价研究，从而评判一个国家或地区、科研单位、高校、期刊、个人的科研产出绩效，来反映其在国际上的学术水平。

### SCI期刊分区
关于JCR(Journal Citation Reports, 期刊引证报告) 期刊分区影响较为广泛的有两种：一种是 Thomson Reuters 公司制定的分区（中文一般翻译为汤森路透分区）；第二种是中国科学院国家科学图书馆制定的分区（简称中科院分区）。这两种分区方式均基于 SCI 收录期刊影响因子基础之上进行分区的。
**1.1 汤森路透分区**
汤森路透每年出版一本《期刊引用报告》（Journal Citation Reports，简称JCR）。JCR对86000多种SCI期刊的影响因子（Impact Factor）等指数加以统计。JCR将收录期刊分为176个不同学科类别。每个学科分类按照期刊的影响因子高低，平均分为Q1、Q2、Q3和Q4四个区：各学科分类中影响因子前25%(含25%)期刊划分为Q1区，前25%～50% (含50%)为Q2区，前50%～75% (含75% )为Q3区， 75%之后的为Q4区。汤森路透分区中期刊的数量是均匀分为四个部分。

**1.2 中科院分区**
中科院首先将JCR中所有期刊分为数学、物理、化学、生物、地学、天文、工程技术、医学、环境科学、农林科学、社会科学、管理科学及综合性期刊13 大类。然后，将13大类期刊分各自为4 个等级，即4 个区。按照各类期刊影响因子划分，前5% 为该类1 区、6% ～ 20% 为2 区、21% ～ 50% 为3 区，其余的为4 区。显然在中科院的分区中，1区和2区杂志很少，杂志质量相对也高，基本都是本领域的顶级期刊。中科院分区中四个区的期刊数量是从1区到4区呈金字塔状分布。
**1.3 汤森路透期刊分区与中科院分区的比较**
中科院期刊分区表常用1-4区，且分区前常用大类或者小类，常用说法为某本期刊在大类某学科为某区。而汤森路透期刊分区常用Q1-Q4（Q表示Quartile in Category），即4个等级中所处的位置，常用说法为某本期刊位于某学科的Q几。期刊Nature的2014的JCR等级情况：期刊Nature位于MULTIDISCIPLINARY SCIENCES学科的Q1。
两者最大的不同在于分区方法。在中科院期刊分区表中，主要参考3年平均IF作为学术影响力，最终每个分区的期刊累积学术影响力是相同的，各区的期刊数量由高到底呈金字塔式分布；在JCR的Journal Ranking中，主要参考当年IF，最终每个分区的期刊数量是均分的。
在国内，大部分高校和科研单位都采用中科院的分区，有些单位有自己定制的分区。显然中科院的1区和2区杂志很少，杂志质量相对也高，基本都是本领域的顶级期刊。由于汤森路透期刊分区中Q1范围的期刊数量多于中科院分区表中的1区期刊，所以有的人发表的论文按汤森路透期刊分区标准算一区，而按中科院分区算2区甚至三区

### 一、术语解释

##### 1. multi-scale和single-scale

Single scale 就是指一个图片送给CNN一个图片，对应的Multi scale 一般是会送给CNN十张图片，比如高宽是256***256****的图片，multi scale 则会在四个角和中心crop 5个224\***224，再把五个crop的图片flip（翻转） ,总共得到十个图片送给CNN。

##### 2. IID: 独立同分布 (independently and identically distributed, IID)

### 二、网络结构推导过程

#### 2.1 数据处理

* 数据集：mnist数据集

* 输入数据：

```python
[imgs_train, labels_train, imgs_test, labels_test] = load_mnist.data_Load()  # 读取训练集与测试集
```

imgs_train: 60000*784

labels_train:60000*1

imgs_test:10000*784

labels_test:10000*1

* 参数

```python
    iters = 1000;  # 训练次数
    alpha = 0.5;  # 学习率
    regulization_rate = 0.1;  # 惩罚系数
    k = 10;  # 标签类别数
    num_train = 2000  # 样本数量
    initial_theta = np.zeros((k, 784))  #initial_theta是10*784
    
    '''
    x是（m行n列）, y是（m行k列）,theta是（k行n列）
    x: 2000*784
    y: 2000*10
    theta: 10*784
    '''
```

* 输入数据处理

训练数据的处理：

```python
    # 将灰度图转化为二值图
    x[x <= 40] = 0;
    x[x > 40] = 1;
```

(20, 40, 80)  => (0, 0, 1)

标签的处理：

```python
    y = np.zeros((num_train, 10))
    # mnist每个样本的标签是一个数字，以独热码的形式将其扩充为10维的向量
    for t in np.arange(0, num_train):
        y[t, labels_train[t]] = 1;
    y = y.T
```

(1) => (1, 0, 0, 0, 0, 0, 0, 0, 0)

(2) => (0, 1, 0, 0, 0, 0, 0, 0, 0)

> ps: 此处y最后进行了转置，y为10 * 2000，x为2000 * 784

#### 2.2 softmax函数

由于 y 有k个类别，故变成一个k维的向量，来表示这个k个类别估计的概率值, 并归一化（概率和为1）。

![](.\人工智能学习笔记\20181009173942846.png)

```python
def softmax(z):
      [m,k]= z.shape  # m = 2000, k = 10
      p = np.zeros([m,k])
      for i in range(m):
          p[i,:]=np.exp(z[i,:])/np.sum(np.exp(z[i,:]))
      return p
```

>p is 2000*10

#### 2.3 cost函数

将`logistics回归`的 0-1 整数规划思想（二维），拓展为 k维 , 引入一个指示函数 `1{·}`

其取值规则为： `1{表达式为真} = 1`

利用指示函数，构造出了代价函数：

![](.\人工智能学习笔记\20181009174028957.png)

```python
def cost(theta, x,y):  #x(m行n列），y（m行k列），theta（k行n列）
      [k,m]=y.shape
      theta = np.matrix(theta)
      sum = 0
      p = softmax(np.dot(x,theta.T))  ## [m,k]     P(i)为 [1,k]，p是2000*10
      p = p.T.reshape([k*m,1])
      y = y.reshape([k*m,1])
      temp_p=np.mat(np.log(p)) # mat与matrix等同
      #punish = np.sum(np.power(theta,2))   #惩罚项，在梯度下降时求导，这里不必加上去
      cost = -1/m*np.dot(y.T,temp_p) #+ punish
      #y.T是1*(k*m),temp_p是(k*m)*1
      return cost         #输出m行k列，代表m个样本，k个类别各自概率?????，不应该是输出一个数？cost应该是一个数值吧
```
#### 2.4 梯度下降方法比较 

使用梯度下降的方法更新参数，也就是当前参数等于上一时刻参数减去学习率乘以梯度。
$$
\theta _{j} = \theta _{j} - \alpha \frac{\partial}{\partial \theta _{j}} J(\theta)
$$
三种方法的不同体现在计算$\frac{\partial}{\partial \theta_j}J(\theta)$的不同上。

假设损失函数为二次函数，那么参数$\theta_j$的更新公式为：
$$
\begin{align*}
\frac{\partial}{\partial \theta _{j}}J(\theta _{j})
&=\frac{\partial}{\partial \theta _{j}} \frac{1}{2}(h_{\theta}(x)-y)^{2}\\
&=2 \cdot \frac{1}{2}(h_{\theta}(x)-y)\cdot\frac{\partial}{\partial\theta_j}(h_{\theta}(x)-y)\\
&=(h_{\theta}(x)-y)\cdot\frac{\partial}{\partial\theta_j}(\sum_{i=0}^{n}\theta_{i}x_{i}-y)\\
&=(h_{\theta}(x)-y)x_{j}
\end{align*}
$$


##### 2.4.1 批量梯度下降(Batch Gradient Descent, BGD)

计算梯度的时候用所有的数据来计算，但是要取平均（不平均就没有意义了）
$$
\frac{\partial J(\theta)}{\partial \theta_{j}} = -\frac{1}{m}\sum_{i=1}^{m}(y^{i} - h_{\theta}(x^{i}))x_{j}^{i}
$$
好处在于收敛次数少，坏处就是每次迭代需要用到所有数据，占用内存大耗时大。

收敛图如下，可以看到收敛的比较快。

![收敛图](pictures/20190625080220317.png)

其中，代价函数的梯度经过公式推导为：

![img](pictures/20181009175629537)

```python
    [finalTheta, finalCost] = gradientDescent(x, y, initial_theta, iters, alpha, regulization_rate)

def gradientDescent(x, y, theta, iters, alpha, regulization_rate):
    # theta:权重系数    iters:迭代次数    alpha:学习率
    COST = np.zeros((iters, 1))  # 存放每次迭代后，cost值的变化
    print(x.shape)
    print(y.shape)
    thetaNums = int(theta.shape[0])  # 维数，即j的取值个数
    print(thetaNums)
    m = y.shape[1]
    for i in range(iters):
        # bb = x*theta.T
        p = softmax(np.dot(x, theta.T));
        grad = (-1 / m * np.dot(x.T, (y.T - p))).T  # [10,784]
        # 更新theta
        theta = theta - alpha * grad  # - regulization_rate * theta
        COST[i] = cost(theta, x, y)
        # 每训练一次，输出当前训练步数与损失值
        print("训练次数： ", i)
        print(COST[i])
        print("\n")
    # 返回迭代后的theta值，和每次迭代的代价函数值
    return theta, COST
```

##### 2.4.2 随机梯度下降（Stochastic Gradient Descent，SGD）

每次只用一个样本求梯度，

![在这里插入图片描述](pictures/20190625080633780.png)

优点是速度快，缺点是可能陷入局部最优，搜索起来比较盲目，并不是每次都朝着最优的方向（因为单个样本可能噪声比较多）。

##### 2.4.3 小批量梯度下降（Mini-batch Gradient Descent，MBGD）

SGD和BGD是两个极端，而MBGD是两种方法的折中，每次选择一批数据来求梯度。

该方法也容易陷入局部最优。

现在说SGD一般都指MBGD。

小批量梯度下降法在BGD算法和SGD算法之间找了一个trade-off，即加快更新速度，并减少噪声的影响，从而减少训练时间和提高准确率。

特点：每次不选择所有的样本也不只选择一个样本，而是选择l（L的小写）个样本，也即bach_size。

则Loss Function函数公式：
$$
\begin{align*}
Loss(\theta)
&=L(\theta_{1}^{i-1},\theta_{2}^{i-1},...,\theta_{j}^{i-1},...,\theta_{n}^{i-1})\\
&=\frac{1}{2l}\sum_{k=1}^{l}(h_{\theta_{1},\theta_{2},...,\theta_{j},...,\theta_{n}}(x^{k})-y^{k})^2
\end{align*}
$$
利用loss函数来更新权重参数的公式：
$$
\begin{align*}
\theta_{j}^{i}
&=\theta_{j}^{i-1}-\eta\frac{\partial}{\partial\theta_{j}^{i-1}}L(\theta_{1}^{i-1},\theta_{2}^{i-1},...,\theta_{j}^{i-1},...,\theta_{n}^{i-1})\\
&=\theta_{j}^{i-1}-\eta\frac{1}{l}\sum_{k=1}^{l}\frac{\partial}{\partial\theta_{j}^{i-1}}(h_{\theta_{1}^{i-1},\theta_{2}^{i-1},...,\theta_{j}^{i-1},...,\theta_{n}^{i-1}}(x^{k}))
\end{align*}
$$

##### 2.4.4 总结

>**BGD(批量梯度下降）**:更新每一参数都用所有样本更新，m=all，更新100次遍历多有数据100次
>
>**SGD（随机梯度下降）**：更新每一参数都随机选择一个样本更新，m=1
>
>**MBGD（小批量梯度下降）**：更新每一参数都选m个样本平均梯度更新，1<m<all
>
>总结：SGD训练速度快，大样本选择；BGD能得到全局最优解，小样本选择；MBGD综合二者选择。

##### 2.4.5 带动量的随机梯度下降

> 因为SGD只依赖于当前迭代的梯度，十分不稳定，加一个“动量”的话，相当于有了一个惯性在里面，梯度方向不仅与这次的迭代有关，还与之前一次的迭代结果有关。“当前一次效果好的话，就加快步伐；当前一次效果不好的话，就减慢步伐”；而且在局部最优值处，没有梯度但因为还存在一个动量，可以跳出局部最优值。

公式为：
$$
velocity = 0\\
velocity = momentum * velocity + \alpha*gradient\\
\theta _{j} = \theta _{j} - velocity
$$

#### 2.5 预测过程

```python
    num = 0
    # np.argmax返回最大数的索引，对应手写数字的label
    y_predict = np.argmax(np.dot(imgs_test, finalTheta.T), axis=1) # imgs_test is 10000*784，finalTheta is 10*784, y_predict is 10000*1
    for i in range(len(labels_test)):
        if (y_predict[i] == labels_test[i]):
            num = num + 1
    print("准确率为 ：", num / len(labels_test))
```

### 三、知识汇总

#### 3.1 训练集、验证集和测试集的意义

在有监督的机器学习中，经常会说到训练集（train)、验证集（validation）和测试集（test），这三个集合的区分可能会让人糊涂，特别是，有些读者搞不清楚验证集和测试集有什么区别。

1

划分

如果我们自己已经有了一个大的标注数据集，想要完成一个有监督模型的测试，那么通常使用均匀随机抽样的方式，将数据集划分为训练集、验证集、测试集，这三个集合不能有交集，常见的比例是8:1:1，当然比例是人为的。从这个角度来看，三个集合都是同分布的。

如果是做比赛，官方只提供了一个标注的数据集（作为训练集）以及一个没有标注的测试集，那么我们做模型的时候，通常会人工从训练集中划分一个验证集出来。

这时候我们通常不再划分一个测试集，可能的原因有两个：1、比赛方基本都很抠，训练集的样本本来就少；2、我们也没法保证要提交的测试集是否跟训练集完全同分布，因此再划分一个跟训练集同分布的测试集就没多大意义了。

2

参数

有了模型后，训练集就是用来训练参数的，说准确点，一般是用来梯度下降的。而验证集基本是在每个epoch完成后，用来测试一下当前模型的准确率。因为验证集跟训练集没有交集，因此这个准确率是可靠的。那么为啥还需要一个测试集呢？

这就需要区分一下模型的各种参数了。事实上，对于一个模型来说，其参数可以分为**普通参数**和**超参数**。在不引入强化学习的前提下，那么普通参数就是可以被梯度下降所更新的，也就是训练集所更新的参数。

另外，还有超参数的概念，比如网络层数、网络节点数、迭代次数、学习率等等，这些参数不在梯度下降的更新范围内。尽管现在已经有一些算法可以用来搜索模型的超参数，但多数情况下我们还是自己人工根据验证集来调。

3

所以

那也就是说，从狭义来讲，验证集没有参与梯度下降的过程，也就是说是没有经过训练的；但从广义上来看，验证集却参与了一个“人工调参”的过程，我们根据验证集的结果调节了迭代数、调节了学习率等等，使得结果在验证集上最优。因此，我们也可以认为，验证集也参与了训练。

那么就很明显了，我们还需要一个完全没有经过训练的集合，那就是测试集，我们既不用测试集梯度下降，也不用它来控制超参数，只是在模型最终训练完成后，用来测试一下最后准确率。

4

然而

聪明的读者就会类比到，其实这是一个无休止的过程。如果测试集准确率很差，那么我们还是会去调整模型的各种参数，这时候又可以认为测试集也参与训练了。好吧，我们可能还需要一个“测试测试集”，也许还需要“测试测试测试集”...

算了吧，还是在测试集就停止吧。

作者：空白_fc21
链接：https://www.jianshu.com/p/7e032a8aaad5
来源：简书
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

#### 3.2 什么是epoch, iteration, batch_size

（1）batchsize：批大小。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batchsize个样本训练；
（2）iteration：1个iteration等于使用batchsize个样本训练一次；一个迭代 = 一个正向通过+一个反向通过
（3）epoch：1个epoch等于使用训练集中的全部样本训练一次；一个epoch = 所有训练样本的一个正向传递和一个反向传递

举个例子，训练集有1000个样本，batchsize=10，那么：
训练完整个样本集需要：
100次iteration，1次epoch。

但是也有人说在有些文章中epoch和iteration是一个概念。

还有就是随机取batchsize，那么并不能保证所有的样本都被抽到，那么epoch的概念不明确。

#### 3.3 什么时候/为什么使用验证集？

1.

验证集可以用在训练的过程中，一般在训练时，几个epoch结束后跑一次验证集看看效果。(验证得太频繁会影响训练速度)

这样做的第一个好处是，可以及时发现模型或者参数的问题，比如模型在验证集上发散啦、出现很奇怪的结果啦(如无穷大)、mAP不增长或者增长很慢啦等等情况，这时可以及时终止训练，重新调参或者调整模型，而不需要等到训练结束。另外一个好处是验证模型的泛化能力，如果在验证集上的效果比训练集上差很多，就该考虑模型是否过拟合了。同时，还可以通过验证集对比不同的模型。

2.

一般而言，我们会设置几个模型去跑，这时候验证集就用于检测哪个模型效果最好，毕竟验证集有标签，至于测试集更多是为了更加保险，竞赛网站一般会给，但在实际上应用中，验证集效果不错就会觉得模型很好了，这时候可能就会考虑落地了。

3.

用来调整超参数。
比如 模型机构，学习率，不同优化器等等
