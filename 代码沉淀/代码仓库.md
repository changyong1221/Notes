## Python

### 文件

#### hashlib对文件进行md5校验

```python
import hashlib
def sercr(data_path):
    with open(data_path,mode='rb') as f1:
        data = hashlib.md5()
        while True:
            data_read = f1.read(1024)
            if not data_read:
                f.close()
                break
            data.update(data_read)
    return data.hexdigest()
print(sercr('PCIe_PHY_Test_Spec_2019_NCB_4.0_1.0.pdf'))
# MD5: A356068D1242616EFB94B86894804134
# SHA1: 612C4D4315B4536C1020D42928240BF4BFAA0A60
```



#### python读文件

```python
file_path = "../models/global/0.npy"
with open(file_path, mode='rb') as f:
    while True:
        data_read = f.readline()
        if not data_read:
            f.close()
            break
        print(data_read)
```



#### python写文件

```python
file_path = "../models/global/hello.txt"
strs = "Hello world!"
ff = open(file_path, 'w+')
ff.write(strs)
print(ff.name)
ff.close()
```



#### python检查目录或文件是否存在

```python
import os

path_str = "models/model.npy"
if os.path.exists(path_str):
    pass
```



#### python创建目录

```python
import os

path = "test"
if not os.path.exists(path):
    os.makedirs(path)
```



#### python删除文件

```python
import os

path_str = "models/model.npy"
os.remove(path_str)
```



#### python复制文件

```python
import shutil

source_path = "models/global/global_model.npy"
dest_path = "models/client/client_model.npy"
shutil.copyfile(source_path, dest_path)
```



#### python删除目录下的所有文件

```python
import os


def clear_results(dir_path):
    """Clear generated results
    """
    ls = os.listdir(dir_path)
    for file in ls:
        file_path = os.path.join(dir_path, file)
        os.remove(file_path)
    print("All results cleared.")
```



#### python递归删除整个目录和文件

```python
import os
import shutil

if os.path.exists(path):
    shutil.rmtree(path)
```



#### python ThreadPoolExecutor线程池

`Python`中已经有了`threading`模块，为什么还需要线程池呢，线程池又是什么东西呢？在介绍[线程同步的信号量机制](https://www.jianshu.com/p/58ec3e7a1edb)的时候，举得例子是爬虫的例子，需要控制同时爬取的线程数，例子中创建了20个线程，而同时只允许3个线程在运行，但是*20个线程都需要创建和销毁，线程的创建是需要消耗系统资源的*，有没有更好的方案呢？其实只需要三个线程就行了，每个线程各分配一个任务，剩下的任务排队等待，当某个线程完成了任务的时候，排队任务就可以安排给这个线程继续执行。

这就是线程池的思想（当然没这么简单），但是自己编写线程池很难写的比较完美，还需要考虑复杂情况下的线程同步，很容易发生死锁。从`Python3.2`开始，标准库为我们提供了`concurrent.futures`模块，它提供了`ThreadPoolExecutor`和`ProcessPoolExecutor`两个类，实现了对`threading`和`multiprocessing`的进一步抽象（这里主要关注线程池），不仅可以帮我们***自动调度线程\***，还可以做到：

1. 主线程可以获取某一个线程（或者任务的）的状态，以及返回值。
2. 当一个线程完成的时候，主线程能够立即知道。
3. 让多线程和多进程的编码接口一致。



##### submit

```python
from concurrent.futures import ThreadPoolExecutor
import time

# 参数times用来模拟网络请求的时间
def get_html(times):
    time.sleep(times)
    print("get page {}s finished".format(times))
    return times

executor = ThreadPoolExecutor(max_workers=2)
# 通过submit函数提交执行的函数到线程池中，submit函数立即返回，不阻塞
task1 = executor.submit(get_html, (3))
task2 = executor.submit(get_html, (2))
# done方法用于判定某个任务是否完成
print(task1.done())
# cancel方法用于取消某个任务,该任务没有放入线程池中才能取消成功
print(task2.cancel())
time.sleep(4)
print(task1.done())
# result方法可以获取task的执行结果
print(task1.result())

# 执行结果
# False  # 表明task1未执行完成
# False  # 表明task2取消失败，因为已经放入了线程池中
# get page 2s finished
# get page 3s finished
# True  # 由于在get page 3s finished之后才打印，所以此时task1必然完成了
# 3     # 得到task1的任务返回值
```

1. `ThreadPoolExecutor`构造实例的时候，传入`max_workers`参数来设置线程池中最多能同时运行的线程数目。
2. 使用`submit`函数来提交线程需要执行的任务（函数名和参数）到线程池中，并返回该任务的句柄（类似于文件、画图），注意`submit()`不是阻塞的，而是立即返回。
3. 通过`submit`函数返回的任务句柄，能够使用`done()`方法判断该任务是否结束。上面的例子可以看出，由于任务有2s的延时，在`task1`提交后立刻判断，`task1`还未完成，而在延时4s之后判断，`task1`就完成了。
4. 使用`cancel()`方法可以取消提交的任务，如果任务已经在线程池中运行了，就取消不了。这个例子中，线程池的大小设置为2，任务已经在运行了，所以取消失败。如果改变线程池的大小为1，那么先提交的是`task1`，`task2`还在排队等候，这是时候就可以成功取消。
5. 使用`result()`方法可以获取任务的返回值。查看内部代码，发现这个方法是阻塞的。

> 注：**submit方法只能传一个参数**，当目标函数有多个参数时，可以将参数打包成json结构体传入。

##### as_completed

上面虽然提供了判断任务是否结束的方法，但是不能在主线程中一直判断啊。有时候我们是得知某个任务结束了，就去获取结果，而不是一直判断每个任务有没有结束。这是就可以使用`as_completed`方法一次取出所有任务的结果。



```python
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

# 参数times用来模拟网络请求的时间
def get_html(times):
    time.sleep(times)
    print("get page {}s finished".format(times))
    return times

executor = ThreadPoolExecutor(max_workers=2)
urls = [3, 2, 4] # 并不是真的url
all_task = [executor.submit(get_html, (url)) for url in urls]

for future in as_completed(all_task):
    data = future.result()
    print("in main: get page {}s success".format(data))

# 执行结果
# get page 2s finished
# in main: get page 2s success
# get page 3s finished
# in main: get page 3s success
# get page 4s finished
# in main: get page 4s success
```

`as_completed()`方法是一个生成器，在没有任务完成的时候，会阻塞，在有某个任务完成的时候，会`yield`这个任务，就能执行for循环下面的语句，然后继续阻塞住，循环到所有的任务结束。从结果也可以看出，**先完成的任务会先通知主线程**。

##### map

除了上面的`as_completed`方法，还可以使用`executor.map`方法，但是有一点不同。



```python
from concurrent.futures import ThreadPoolExecutor
import time

# 参数times用来模拟网络请求的时间
def get_html(times):
    time.sleep(times)
    print("get page {}s finished".format(times))
    return times

executor = ThreadPoolExecutor(max_workers=2)
urls = [3, 2, 4] # 并不是真的url

for data in executor.map(get_html, urls):
    print("in main: get page {}s success".format(data))
# 执行结果
# get page 2s finished
# get page 3s finished
# in main: get page 3s success
# in main: get page 2s success
# get page 4s finished
# in main: get page 4s success
```

使用`map`方法，无需提前使用`submit`方法，`map`方法与`python`标准库中的`map`含义相同，都是将序列中的每个元素都执行同一个函数。上面的代码就是对`urls`的每个元素都执行`get_html`函数，并分配各线程池。可以看到执行结果与上面的`as_completed`方法的结果不同，**输出顺序和`urls`列表的顺序相同**，就算2s的任务先执行完成，也会先打印出3s的任务先完成，再打印2s的任务完成。

##### wait

`wait`方法可以让主线程阻塞，直到满足设定的要求。

```python
from concurrent.futures import ThreadPoolExecutor, wait, ALL_COMPLETED, FIRST_COMPLETED
import time

# 参数times用来模拟网络请求的时间
def get_html(times):
    time.sleep(times)
    print("get page {}s finished".format(times))
    return times

executor = ThreadPoolExecutor(max_workers=2)
urls = [3, 2, 4] # 并不是真的url
all_task = [executor.submit(get_html, (url)) for url in urls]
wait(all_task, return_when=ALL_COMPLETED)
print("main")
# 执行结果 
# get page 2s finished
# get page 3s finished
# get page 4s finished
# main
```

`wait`方法接收3个参数，等待的任务序列、超时时间以及等待条件。等待条件`return_when`默认为`ALL_COMPLETED`，表明要等待所有的任务都结束。可以看到运行结果中，确实是所有任务都完成了，主线程才打印出`main`。等待条件还可以设置为`FIRST_COMPLETED`，表示第一个任务完成就停止等待。



#### python 执行shell命令

```python
import os
os.system("ls")
```



#### python 外部传参

```python
import argparse

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-n', '--number', dest="number", type=int, default=1)
    args = parser.parse_args()
    clients_num = args.number
    

# 命令行操作
python run.py -n 2
```



#### python查询本机ipv4地址

```python
import socket


def get_host_ip():
    """
    查询本机ip地址
    :return:
    """
    try:
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        s.connect(('8.8.8.8', 80))
        ip = s.getsockname()[0]
    finally:
        s.close()

    return ip


if __name__ == '__main__':
    print(get_host_ip())
```



#### python requests方法总结

requests.post()  方法的使用

https://blog.csdn.net/lmz_lmz/article/details/87632186

requests库：

https://www.jianshu.com/p/fb6ee6cc5c1c

https://blog.csdn.net/shanzhizi/article/details/50903748

requests库官网：

https://docs.python-requests.org/zh_CN/latest/user/quickstart.html#id2



### Flask

#### Flask进行客户端与服务器通信

客户端：

```python
from flask import Flask, render_template
from src.train import train_models
import requests

app = Flask(__name__)
app.config['SECRET_KEY'] = 'abcdefg'


@app.route("/")
def root():
    """
    主页
    :return: Index.html
    """
    return render_template('Index.html')


@app.route("/send")
def send():
    data = {'file': open('log.log', 'rb')}
    response = requests.post('http://10.112.58.204:40000/receive', files=data)

    #response = requests.get('http://10.112.58.204:50000/receive', files=data)
    print(response.text)
    job = {"hasdone": 0}
    return job


if __name__ == '__main__':
    app.run(debug=True, host='127.0.0.1', port='5000')
```



服务器：

```python
from flask import Flask, request
import json, os

app = Flask(__name__)
app.config['SECRET_KEY'] = 'abcdefg'

@app.route("/")
def root():
    return render_template('Index.html')

@app.route("/receive", methods=['POST'])
def receive():
    files = request.files['file']
    save_path = './'
    files.save(os.path.join(save_path, files.filename))
    # get parameters
    #data = request.args.to_dict()
    #string = data.get('string')
    #print(string)
    #ff = request.files['file']
    #save_path = './'
    #ff.save(os.path.join(save_path, ff.filename))
    return json.dumps({'data':'ddd', 'success':'yes'})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port='40000')


```



#### Flask request 属性详解

https://blog.csdn.net/u011146423/article/details/88191225

https://www.cnblogs.com/baijinshuo/p/10235932.html



## Swarm存储

### 以太坊Swarm文件存储说明

https://mp.weixin.qq.com/s/FZDopBEVfeomRWUN-w9dow



### 以太坊Swarm文件存储入门之主网和测试网

https://mp.weixin.qq.com/s/FSqr_LM4EWP96d84yMeEwg



### 以太坊Swarm文件的上传和下载

https://mp.weixin.qq.com/s/m4rR6V-9_Q-ejOjZWYJmkA



# Pandas

#### 设置列名

```python
#method1,直接重新命名df1的列名
df1.columns=['a','B','c']  
print('method1:\n',df1)
```



